{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a957071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import re, string, unicodedata\n",
    "#import inflect\n",
    "\n",
    "import spacy\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "#from __future__ import unicode_literals\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee2a89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>words_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>1</td>\n",
       "      <td>I already had a phone with problems... I know ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "5  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \\\n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0   \n",
       "5       1  I already had a phone with problems... I know ...           1.0   \n",
       "\n",
       "   words_counts  \n",
       "0            72  \n",
       "1            40  \n",
       "3            17  \n",
       "4            43  \n",
       "5            80  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Amazon_Meta_Data = pd.read_csv('G:/PuthonRun/Amazon_Review/data/Amazon_Unlocked_Mobile.csv',nrows=1000, encoding='utf-8')\n",
    "\n",
    "#------- Counting words in reviews ---------\n",
    "word_counts = []\n",
    "for review in Amazon_Meta_Data['Reviews']:\n",
    "    count=0\n",
    "    for word in str(review).split():\n",
    "        count +=1\n",
    "    word_counts.append(count)\n",
    "Amazon_Meta_Data['words_counts'] = word_counts\n",
    "\n",
    "# Discarding rows having less then 5 words of review.\n",
    "Amazon_Meta_Data=Amazon_Meta_Data.loc[Amazon_Meta_Data['words_counts'] >=5]\n",
    "\n",
    "Amazon_Meta_Data.head()\n",
    "#Amazon_Meta_Data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e740c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews = Amazon_Meta_Data['Reviews']\n",
    "Brand_Name = Amazon_Meta_Data['Brand Name'].str.upper()\n",
    "brand_list =list(set(Brand_Name))\n",
    "unique_products=list(set(Amazon_Meta_Data['Product Name']))\n",
    "unique_rating = list(set(Amazon_Meta_Data['Rating']))\n",
    "\n",
    "#**unique products in dataset and total reviews about each product in dataset.**\n",
    "product_count = []\n",
    "for prod in unique_products:\n",
    "    product_count.append(len(Amazon_Meta_Data.loc[Amazon_Meta_Data['Product Name'] == prod]))\n",
    "    \n",
    "\n",
    "#**Detailed information About dataset**\n",
    "# print('total Reviews    : ',len(Reviews))\n",
    "# print('total Brands     : ',len(Brand_Name.value_counts()))\n",
    "# print('total Products   : ',len(product_count))\n",
    "# print('total Brands with their counts are \\n',Brand_Name.value_counts())\n",
    "\n",
    "\n",
    "#**Ratings count according to the product**\n",
    "rating=list(set(Amazon_Meta_Data['Rating']))\n",
    "for rating in unique_rating:\n",
    "    globals()['rating_list_%s' % rating] =[]\n",
    "for prod in unique_products:\n",
    "    for rating in unique_rating:\n",
    "        globals()['rating_list_%s' % rating].append(len(Amazon_Meta_Data.loc[(Amazon_Meta_Data['Product Name'] == prod) & (Amazon_Meta_Data['Rating']==rating)]))\n",
    "\n",
    "        \n",
    "products_dataset = pd.DataFrame()\n",
    "products_dataset['Product Name'] = unique_products\n",
    "products_dataset['Total_Reviews'] = product_count\n",
    "products_dataset['1_Rating'] = rating_list_1\n",
    "products_dataset['2_Rating'] = rating_list_2\n",
    "products_dataset['3_Rating'] = rating_list_3\n",
    "products_dataset['4_Rating'] = rating_list_4\n",
    "products_dataset['5_Rating'] = rating_list_5\n",
    "\n",
    "# products_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1114ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "#Steemming and Lemmatization\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "\n",
    "#stems, lemmas = stem_and_lemmatize(words)\n",
    "#print('Stemmed:\\n', stems)\n",
    "#print('\\nLemmatized:\\n', lemmas)\n",
    "\n",
    "\n",
    "# ---------------   Cleaning   ------------------\n",
    "def clean_text(text):\n",
    "    wording = nltk.word_tokenize(text)\n",
    "    words = normalize(wording)\n",
    "    string_text = ' '.join(words)\n",
    "    return string_text\n",
    "\n",
    "# ---------------   Sentiment   ------------------\n",
    "def get_text_sentiment(text):\n",
    "    # create TextBlob object of passed text \n",
    "    analysis = TextBlob(clean_text(text)) \n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'    \n",
    "    \n",
    "#---------------------- TextBlob Feature Extractions -----------------\n",
    "#Function to extract features from text\n",
    "def textBlob_feature_extraction(text): \n",
    "        blob = TextBlob(text)\n",
    "        return blob.noun_phrases\n",
    "    \n",
    "    \n",
    "#----------------------  extract Sentences  -------------------------    \n",
    "def sentances(text):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return sent_detector.tokenize(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "911eba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Cleaned_Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TextBlob_Features</th>\n",
       "      <th>Spacy_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>feel lucky found used phone us used hard phone...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[phone line, years nt]</td>\n",
       "      <td>line,son,one,seller,seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>nice phone nice grade pantach revue clean set ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[nice phone, nice grade pantach revue, clean s...</td>\n",
       "      <td>pantach,size,media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>works good goes slow sometimes good phone love</td>\n",
       "      <td>positive</td>\n",
       "      <td>[good phone]</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>great phone replace lost phone thing volume bu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[great phone, phone thing volume button work, ...</td>\n",
       "      <td>replace,work,settings,job,againthaanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>I already had a phone with problems... I know ...</td>\n",
       "      <td>already phone problems know stated used dang s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[phone problems, dang state charge, way work, ...</td>\n",
       "      <td>problems,wish,comments,goods,charge,work,money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...   \n",
       "5  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...   \n",
       "\n",
       "                                             Reviews  \\\n",
       "0  I feel so LUCKY to have found this used (phone...   \n",
       "1  nice phone, nice up grade from my pantach revu...   \n",
       "3  It works good but it goes slow sometimes but i...   \n",
       "4  Great phone to replace my lost phone. The only...   \n",
       "5  I already had a phone with problems... I know ...   \n",
       "\n",
       "                                     Cleaned_Reviews Sentiment  \\\n",
       "0  feel lucky found used phone us used hard phone...  positive   \n",
       "1  nice phone nice grade pantach revue clean set ...  positive   \n",
       "3     works good goes slow sometimes good phone love  positive   \n",
       "4  great phone replace lost phone thing volume bu...  positive   \n",
       "5  already phone problems know stated used dang s...  negative   \n",
       "\n",
       "                                   TextBlob_Features  \\\n",
       "0                             [phone line, years nt]   \n",
       "1  [nice phone, nice grade pantach revue, clean s...   \n",
       "3                                       [good phone]   \n",
       "4  [great phone, phone thing volume button work, ...   \n",
       "5  [phone problems, dang state charge, way work, ...   \n",
       "\n",
       "                                      Spacy_features  \n",
       "0                         line,son,one,seller,seller  \n",
       "1                                 pantach,size,media  \n",
       "3                                               love  \n",
       "4             replace,work,settings,job,againthaanks  \n",
       "5  problems,wish,comments,goods,charge,work,money...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------  Text Cleaning & Sentiment Extraction --------------------\n",
    "cleaned_reviews =[]\n",
    "sentiment= []\n",
    "for reviews in Amazon_Meta_Data['Reviews']:\n",
    "    cleaned_reviews.append(clean_text(reviews))\n",
    "    sentiment.append(get_text_sentiment(reviews))\n",
    "    \n",
    "features_Dataset = pd.DataFrame()\n",
    "features_Dataset['Product Name'] = Amazon_Meta_Data['Product Name']\n",
    "features_Dataset['Reviews'] = Amazon_Meta_Data['Reviews']\n",
    "features_Dataset['Cleaned_Reviews'] = cleaned_reviews\n",
    "features_Dataset['Sentiment'] = sentiment\n",
    "\n",
    "\n",
    "# Extracting Features from each review using TextBlob & Spacy *\n",
    "    \n",
    "#--------------------- Text Blob -------------------------\n",
    "features = []\n",
    "for reviews in features_Dataset['Cleaned_Reviews']:\n",
    "    features.append(textBlob_feature_extraction(reviews))\n",
    "#adding Extracted features to dataset\n",
    "features_Dataset[\"TextBlob_Features\"] = features\n",
    "\n",
    "#--------------------- Spacy -----------------------------\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "feature_spacy = []\n",
    "for review in nlp.pipe(features_Dataset['Cleaned_Reviews']):\n",
    "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    feature_spacy.append(','.join(chunks))\n",
    "\n",
    "features_Dataset['Spacy_features']= feature_spacy\n",
    "\n",
    "#---------------------------------------------------------\n",
    "features_Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2735fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob features extracted from dataset are :  2582\n",
      "TextBlob features left after discarding those who occure less then three times in dataset are :  333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=feature_TextBlob.csv>Download features and counts as CSV file</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------- Features Counts ------------------\n",
    "#function to extract Features With their counts\n",
    "counts = {}\n",
    "def feature_count(feature):\n",
    "    if  feature in counts:\n",
    "        counts[feature] += 1\n",
    "    else:\n",
    "        counts[feature] = 1\n",
    "#Extracting feature count form feartures\n",
    "for feature_list in features_Dataset['TextBlob_Features']:\n",
    "    for feature in feature_list:\n",
    "        feature_count(feature)\n",
    "\n",
    "#-----------------Distinct Features Counts ------------------\n",
    "#dataframe of features and their count    \n",
    "dist_feature =[]\n",
    "dist_feature_count= []\n",
    "for key, value in counts.items() :\n",
    "    dist_feature.append(key)\n",
    "    dist_feature_count.append(value)\n",
    "    \n",
    "#----------------- Features Counts Dataset ------------------\n",
    "feature_TextBlob = pd.DataFrame()\n",
    "feature_TextBlob['Feature Name'] =dist_feature\n",
    "feature_TextBlob['Feature Count'] =dist_feature_count\n",
    "\n",
    "#----------------- Features Details ------------------\n",
    "#discarding feature that occure less the 3 in the entire dataset\n",
    "print('TextBlob features extracted from dataset are : ' ,len(feature_TextBlob))\n",
    "feature_TextBlob =feature_TextBlob.loc[feature_TextBlob['Feature Count'] >=3]\n",
    "print('TextBlob features left after discarding those who occure less then three times in dataset are : ' ,len(feature_TextBlob))\n",
    "#feature_TextBlob\n",
    "\n",
    "#--------------------  Downlaodable CVS of Features and their count --------------------\n",
    "from IPython.display import HTML\n",
    "feature_TextBlob.to_csv('feature_TextBlob.csv', index=False)\n",
    "\n",
    "def create_download_link(title = \"Download features and counts as CSV file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a link to download the dataframe which was saved with .to_csv method\n",
    "create_download_link(filename='feature_TextBlob.csv')\n",
    "##==========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e751b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
